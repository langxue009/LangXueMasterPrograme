{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file paths on your local machine\n",
    "# Change this line later on your python script when you want to run this on the CLOUD (GC or AWS)\n",
    "\n",
    "wikiPagesFile=\"WikipediaPagesOneDocPerLine1000LinesSmall.txt\"\n",
    "wikiCategoryFile=\"wiki-categorylinks-small.csv.bz2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[*]\", \"Assignment2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"434042\",\"1987_debut_albums\"', '\"434042\",\"Albums_produced_by_Mike_Varney\"']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read two files into RDDs\n",
    "\n",
    "wikiCategoryLinks=sc.textFile(wikiCategoryFile)\n",
    "\n",
    "wikiCats=wikiCategoryLinks.map(lambda x: x.split(\",\")).map(lambda x: (x[0].replace('\"', ''), x[1].replace('\"', '') ))\n",
    "\n",
    "# Now the wikipages\n",
    "\n",
    "wikiPages = sc.textFile(wikiPagesFile)\n",
    "\n",
    "wikiCategoryLinks.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('434042', '1987_debut_albums'), ('434042', 'Albums_produced_by_Mike_Varney')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiCats.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Assignment2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(wikiPagesFile)\n",
    "\n",
    "# Uncomment this line if you want to take look inside the file. \n",
    "# df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikiPages.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Assumption: Each document is stored in one line of the text file\n",
    "# We need this count later ... \n",
    "numberOfDocs = wikiPages.count()\n",
    "\n",
    "print(numberOfDocs)\n",
    "# Each entry in validLines will be a line from the text file\n",
    "validLines = wikiPages.filter(lambda x : 'id' in x and 'url=' in x)\n",
    "\n",
    "# Now, we transform it into a set of (docID, text) pairs\n",
    "keyAndText = validLines.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6])) \n",
    "\n",
    "# keyAndText.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildArray(listOfIndices):\n",
    "    \n",
    "    returnVal = np.zeros(20000)\n",
    "    \n",
    "    for index in listOfIndices:\n",
    "        returnVal[index] = returnVal[index] + 1\n",
    "    \n",
    "    mysum = np.sum(returnVal)\n",
    "    \n",
    "    returnVal = np.divide(returnVal, mysum)\n",
    "    \n",
    "    return returnVal\n",
    "\n",
    "\n",
    "def build_zero_one_array (listOfIndices):\n",
    "    \n",
    "    returnVal = np.zeros (20000)\n",
    "    \n",
    "    for index in listOfIndices:\n",
    "        if returnVal[index] == 0: returnVal[index] = 1\n",
    "    \n",
    "    return returnVal\n",
    "\n",
    "\n",
    "def stringVector(x):\n",
    "    returnVal = str(x[0])\n",
    "    for j in x[1]:\n",
    "        returnVal += ',' + str(j)\n",
    "    return returnVal\n",
    "\n",
    "\n",
    "\n",
    "def cousinSim (x,y):\n",
    "\tnormA = np.linalg.norm(x)\n",
    "\tnormB = np.linalg.norm(y)\n",
    "\treturn np.dot(x,y)/(normA*normB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we transform it into a set of (docID, text) pairs\n",
    "keyAndText = validLines.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\n",
    "\n",
    "# Now, we split the text in each (docID, text) pair into a list of words\n",
    "# After this step, we have a data set with\n",
    "# (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# We use a regular expression here to make\n",
    "# sure that the program does not break down on some of the documents\n",
    "\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "# remove all non letter characters\n",
    "keyAndListOfWords = keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
    "# better solution here is to use NLTK tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 1), ('people', 1)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# to (\"word1\", 1) (\"word2\", 1)...\n",
    "allWords = keyAndListOfWords.flatMap(lambda x: x[1]).map(lambda x: (x, 1))\n",
    "allWords.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('at', 4810), ('two', 1848)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
    "allCounts = allWords.reduceByKey(lambda x,y: x+y)\n",
    "allCounts.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Words in Corpus: [('the', 74530), ('of', 34512), ('and', 28479), ('in', 27758), ('to', 22583), ('a', 21212), ('was', 12160), ('as', 8811), ('for', 8773), ('on', 8435)]\n"
     ]
    }
   ],
   "source": [
    "# Get the top 20,000 words in a local array in a sorted format based on frequency\n",
    "# If you want to run it on your laptio, it may a longer time for top 20k words. \n",
    "topWords = allCounts.top(20000, key=lambda x: x[1])\n",
    "# \n",
    "print(\"Top Words in Corpus:\", allCounts.top(10, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Postions in our Feature Matrix. Last 20 words in 20k positions:  [('quebecor', 19999), ('poten', 19998), ('kasada', 19997), ('yadnya', 19996), ('drift', 19995), ('iata', 19994), ('satire', 19993), ('expreso', 19992), ('olimpico', 19991), ('auxiliaries', 19990), ('tenses', 19989), ('petherick', 19988), ('stowe', 19987), ('infimum', 19986), ('parramatta', 19985), ('rimpac', 19984), ('hyderabad', 19983), ('cubes', 19982), ('meats', 19981), ('chaat', 19980)]\n"
     ]
    }
   ],
   "source": [
    "# We'll create a RDD that has a set of (word, dictNum) pairs\n",
    "# start by creating an RDD that has the number 0 through 20000\n",
    "# 20000 is the number of words that will be in our dictionary\n",
    "topWordsK = sc.parallelize(range(20000))\n",
    "\n",
    "# Now, we transform (0), (1), (2), ... to (\"MostCommonWord\", 1)\n",
    "# (\"NextMostCommon\", 2), ...\n",
    "# the number will be the spot in the dictionary used to tell us\n",
    "# where the word is located\n",
    "dictionary = topWordsK.map (lambda x : (topWords[x][0], x))\n",
    "\n",
    "print(\"Word Postions in our Feature Matrix. Last 20 words in 20k positions: \", dictionary.top(20, lambda x : x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### TASK 2  ##################\n",
    "\n",
    "# Next, we get a RDD that has, for each (docID, [\"word1\", \"word2\", \"word3\", ...]),\n",
    "# (\"word1\", docID), (\"word2\", docId), ...\n",
    "\n",
    "allWordsWithDocID = keyAndListOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now join and link them, to get a set of (\"word1\", (dictionaryPos, docID)) pairs\n",
    "allDictionaryWords = dictionary.join(allWordsWithDocID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', (1, '431949')), ('of', (1, '431949')), ('of', (1, '431949'))]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allDictionaryWords.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('431949', 1), ('431949', 1), ('431949', 1)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we drop the actual word itself to get a set of (docID, dictionaryPos) pairs\n",
    "justDocAndPos = allDictionaryWords.map(lambda x: (x[1][1],x[1][0]))\n",
    "justDocAndPos.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('431971', array([0.08553655, 0.02488336, 0.05132193, ..., 0.        , 0.        ,\n",
      "       0.        ])), ('431999', array([0.06479482, 0.03239741, 0.03887689, ..., 0.        , 0.        ,\n",
      "       0.        ])), ('432000', array([0.07192043, 0.04475899, 0.02371844, ..., 0.        , 0.        ,\n",
      "       0.        ]))]\n"
     ]
    }
   ],
   "source": [
    "# Now get a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "allDictionaryWordsInEachDoc = justDocAndPos.groupByKey()\n",
    "\n",
    "\n",
    "# The following line this gets us a set of\n",
    "# (docID,  [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "# and converts the dictionary positions to a bag-of-words numpy array...\n",
    "allDocsAsNumpyArrays = allDictionaryWordsInEachDoc.map(lambda x: (x[0], buildArray(x[1])))\n",
    "\n",
    "print(allDocsAsNumpyArrays.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([919, 920, 869, ...,   1,   1,   1])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, create a version of allDocsAsNumpyArrays where, in the array,\n",
    "# every entry is either zero or one.\n",
    "# A zero means that the word does not occur,\n",
    "# and a one means that it does.\n",
    "\n",
    "zeroOrOne = allDocsAsNumpyArrays.map(lambda x: (x[0], 1*(x[1] > 0)))\n",
    "\n",
    "# Now, add up all of those arrays into a single array, where the\n",
    "# i^th entry tells us how many\n",
    "# individual documents the i^th word in the dictionary appeared in\n",
    "dfArray = zeroOrOne.reduce(lambda x1, x2: (\"\", np.add(x1[1], x2[1])))[1]\n",
    "dfArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of 20,000 entries, each entry with the value numberOfDocs (number of docs)\n",
    "multiplier = np.full(20000, numberOfDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('431971',\n",
       "  array([0.0072252 , 0.00207481, 0.00720622, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('431999',\n",
       "  array([0.00547316, 0.00270135, 0.00545879, ..., 0.        , 0.        ,\n",
       "         0.        ]))]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the version of dfArray where the i^th entry is the inverse-document frequency for the\n",
    "# i^th word in the corpus\n",
    "idfArray = np.log(np.divide(np.full(20000, numberOfDocs), dfArray))\n",
    "\n",
    "# Finally, convert all of the tf vectors in allDocsAsNumpyArrays to tf * idf vectors\n",
    "allDocsAsNumpyArraysTFidf = allDocsAsNumpyArrays.map(lambda x: (x[0], np.multiply(x[1], idfArray)))\n",
    "\n",
    "allDocsAsNumpyArraysTFidf.take(2)\n",
    "\n",
    "# use the buildArray function to build the feature array\n",
    "# allDocsAsNumpyArrays = allDictionaryWordsInEachDoc.map(lambda x: (x[0], buildArray(x[1])))\n",
    "\n",
    "\n",
    "# print(allDocsAsNumpyArraysTFidf.take(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('434042', '1987_debut_albums')]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiCats.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Asteroid_spectral_classes',\n",
       "  array([0.00674469, 0.00348744, 0.00373721, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('S-type_asteroids',\n",
       "  array([0.00674469, 0.00348744, 0.00373721, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('All_stub_articles',\n",
       "  array([0.00787168, 0.00328745, 0.00754904, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('Military_communications_of_the_United_States',\n",
       "  array([0.00787168, 0.00328745, 0.00754904, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('United_States_Department_of_Defense_agencies',\n",
       "  array([0.00787168, 0.00328745, 0.00754904, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('United_States_military_stubs',\n",
       "  array([0.00787168, 0.00328745, 0.00754904, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('2004_European_Parliament_election',\n",
       "  array([0.00711319, 0.0026331 , 0.00394139, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('All_articles_that_may_contain_original_research',\n",
       "  array([0.00711319, 0.0026331 , 0.00394139, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('All_articles_with_dead_external_links',\n",
       "  array([0.00711319, 0.0026331 , 0.00394139, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('Articles_that_may_contain_original_research_from_September_2007',\n",
       "  array([0.00711319, 0.0026331 , 0.00394139, ..., 0.        , 0.        ,\n",
       "         0.        ]))]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we join it with categories, and map it after join so that we have only the wikipageID \n",
    "# This join can take time on your laptop. \n",
    "# You can do the join once and generate a new wikiCats data and store it. Our WikiCategories includes all categories\n",
    "# of wikipedia. \n",
    "\n",
    "featuresRDD = wikiCats.join(allDocsAsNumpyArraysTFidf).map(lambda x: (x[1][0], x[1][1]))\n",
    "\n",
    "# Cache this important data because we need to run kNN on this data set. \n",
    "featuresRDD.cache()\n",
    "featuresRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13780"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us count and see how large is this data set. \n",
    "\n",
    "#wikiAndCatsJoind.count()\n",
    "featuresRDD.count()\n",
    "\n",
    "#*** do you mean to check the size of featuresRDD instead of wikiAndCatsJoind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Asteroid_spectral_classes',\n",
       "  array([0.00674469, 0.00348744, 0.00373721, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('S-type_asteroids',\n",
       "  array([0.00674469, 0.00348744, 0.00373721, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('All_stub_articles',\n",
       "  array([0.00787168, 0.00328745, 0.00754904, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('Military_communications_of_the_United_States',\n",
       "  array([0.00787168, 0.00328745, 0.00754904, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('United_States_Department_of_Defense_agencies',\n",
       "  array([0.00787168, 0.00328745, 0.00754904, ..., 0.        , 0.        ,\n",
       "         0.        ]))]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we have a function that returns the prediction for the label of a string, using a kNN algorithm\n",
    "def getPrediction (textInput, k):\n",
    "    # Create an RDD out of the textIput\n",
    "    myDoc = sc.parallelize (('', textInput))\n",
    "\n",
    "    # Flat map the text to (word, 1) pair for each word in the doc\n",
    "    wordsInThatDoc = myDoc.flatMap (lambda x : ((j, 1) for j in regex.sub(' ', x).lower().split()))\n",
    "\n",
    "    # This will give us a set of (word, (dictionaryPos, 1)) pairs\n",
    "    allDictionaryWordsInThatDoc = dictionary.join (wordsInThatDoc).map (lambda x: (x[1][1], x[1][0])).groupByKey ()\n",
    "\n",
    "    # Get tf array for the input string\n",
    "    myArray = buildArray (allDictionaryWordsInThatDoc.top (1)[0][1])\n",
    "\n",
    "    # Get the tf * idf array for the input string\n",
    "    myArray = np.multiply (myArray, idfArray)\n",
    "\n",
    "    # Get the distance from the input text string to all database documents, using cosine similarity (np.dot() )\n",
    "    distances = featuresRDD.map (lambda x : (x[0], np.dot (x[1], myArray)))\n",
    "    # distances = allDocsAsNumpyArraysTFidf.map (lambda x : (x[0], cousinSim (x[1],myArray)))\n",
    "    # get the top k distances\n",
    "    topK = distances.top (k, lambda x : x[1])\n",
    "    \n",
    "    # and transform the top k distances into a set of (docID, 1) pairs\n",
    "    docIDRepresented = sc.parallelize(topK).map (lambda x : (x[0], 1))\n",
    "\n",
    "    # now, for each docID, get the count of the number of times this document ID appeared in the top k\n",
    "    numTimes = docIDRepresented.reduceByKey(lambda x,y: x+y)\n",
    "    \n",
    "    # Return the top 1 of them.\n",
    "    # Ask yourself: Why we are using twice top() operation here?\n",
    "    return numTimes.top(k, lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1931_births', 1), ('All_disambiguation_pages', 1), ('2015_deaths', 1), ('Disambiguation_pages_with_short_description', 1), ('Bullfighters', 1), (\"Air_Force_Falcons_men's_basketball_coaches\", 1), ('Human_name_disambiguation_pages', 1), ('All_article_disambiguation_pages', 1), ('Lists_of_sportspeople_by_sport', 1), ('All_articles_with_dead_external_links', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(getPrediction('Sport Basketball Volleyball Soccer', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('All_Wikipedia_articles_written_in_Australian_English', 2), ('All_set_index_articles', 2), ('Articles_with_short_description', 2), ('Royal_Australian_Navy_ship_names', 1), ('Set_indices_on_ships', 1), ('Use_Australian_English_from_April_2018', 1), ('Use_dmy_dates_from_April_2018', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(getPrediction('What is the capital city of Australia?', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('All_stub_articles', 2), ('CBC_Television_shows', 1), ('1979_births', 1), ('1990s_Canadian_teen_drama_television_series', 1), ('1991_Canadian_television_series_debuts', 1), ('1994_Canadian_television_series_endings', 1), ('Canadian_television_program_stubs', 1), ('Television_shows_set_in_Vancouver', 1), ('Ak_Bars_Kazan_players', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(getPrediction('How many goals Vancouver score last year?', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congradulations, you have implemented a prediction system based on Wikipedia data. \n",
    "# You can use this system to generate automated Tags or Categories for any kind of text \n",
    "# that you put in your query.\n",
    "# This data model can predict categories for any input text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Using Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv(wikiCategoryFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   _c0|count|\n",
      "+------+-----+\n",
      "|455070|   20|\n",
      "|434061|    8|\n",
      "|455037|    4|\n",
      "|455790|    9|\n",
      "|456150|   14|\n",
      "|429120|    3|\n",
      "|429107|   15|\n",
      "|433861|   15|\n",
      "|434000|   17|\n",
      "|429031|   34|\n",
      "|418448|    6|\n",
      "|456178|    4|\n",
      "|429164|   40|\n",
      "|455361|   12|\n",
      "|433488|   32|\n",
      "|455962|   10|\n",
      "|433664|   23|\n",
      "|432147|   48|\n",
      "|429110|   14|\n",
      "|432184|   10|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of categories used for each wikipage\n",
    "df2.groupBy('_c0').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             count|\n",
      "+-------+------------------+\n",
      "|  count|               999|\n",
      "|   mean|13.793793793793794|\n",
      "| stddev| 11.06201871439371|\n",
      "|    min|                 1|\n",
      "|    max|                76|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy('_c0').count().select(\"count\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 _c1|count|\n",
      "+--------------------+-----+\n",
      "|Articles_with_sho...|  273|\n",
      "|All_articles_with...|  222|\n",
      "|Wikipedia_article...|  212|\n",
      "|Wikipedia_article...|  212|\n",
      "|Wikipedia_article...|  196|\n",
      "|Commons_category_...|  178|\n",
      "|Wikipedia_article...|  157|\n",
      "|Wikipedia_article...|  155|\n",
      "|All_articles_need...|  154|\n",
      "|Webarchive_templa...|  140|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 mostly used wikipedia categories\n",
    "df2.groupBy('_c1').count().sort(\"count\", ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Removing Stop Words, do Stemming and redo the task 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1 - Remove Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we could possibly have different prediction results if removing the stop words. However, I don't think the result will change heavily. Yes, the stop words may appear more frequenctly in a document. At first, we did measure how frequenctly a word appears in a document. However, we eventually change it to 0 and 1. A zero means that the word does not occur, and a one means that it does. Therefore, even the stop words appear many times for a particularly document, we only count it 1, means it does appear in the document. It has the same weight as a word that appear only one time in the document. Therefore, removing the stop words do not bring a huge impact in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2 - Do English word stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. I think stemming will change the result heavily. For example, if the input text has a word \"games\". In the dictionary, we only has the word \"game\" not \"games\". In Task2, the function considers the word \"game\" never appears in the input text. It has big impact than previous situation. In previous situation, a stop word may appear many times but we still count it to appear one time. But if a word consider not appear at all, it will not take into account in distance calculation toward a particular category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
