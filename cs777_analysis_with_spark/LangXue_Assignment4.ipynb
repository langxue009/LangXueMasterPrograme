{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://puddings-mbp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freqArray (listOfIndices, numberofwords):\n",
    "    returnVal = np.zeros (20000)\n",
    "    for index in listOfIndices:\n",
    "        returnVal[index] = returnVal[index] + 1\n",
    "    returnVal = np.divide(returnVal, numberofwords)\n",
    "    return returnVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_corpus = sc.textFile(\"/Users/pudding/Downloads/SmallTrainingData.txt\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we transform it into a set of (docID, text) pairs\n",
    "d_keyAndText = d_corpus.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all non letter characters\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "d_keyAndListOfWords = d_keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('consideration', 1), ('of', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change (docID, [\"word1\", \"word2\", \"word3\", ...]) to (\"word1\", 1) (\"word2\", 1)\n",
    "allWords = d_keyAndListOfWords.flatMap(lambda x: x[1]).map(lambda x: (x, 1))\n",
    "allWords.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
    "allCounts = allWords.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 20,000 words in a local array in a sorted format based on frequency\n",
    "# If you want to run it on your laptio, it may a longer time for top 20k words. \n",
    "topWords = allCounts.top(20000, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 447995),\n",
       " ('of', 205873),\n",
       " ('and', 177099),\n",
       " ('in', 158991),\n",
       " ('to', 144197)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topWords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fiqh', 17),\n",
       " ('sirte', 17),\n",
       " ('chittagong', 17),\n",
       " ('odisha', 17),\n",
       " ('morelos', 17)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topWords[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create a RDD that has a set of (word, dictNum) pairs\n",
    "# start by creating an RDD that has the number 0 through 20000\n",
    "# 20000 is the number of words that will be in our dictionary\n",
    "topWordsK = sc.parallelize(range(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Postions in our Feature Matrix. Last 20 words in 20k positions:  [('morelos', 19999), ('odisha', 19998), ('chittagong', 19997), ('sirte', 19996), ('fiqh', 19995), ('sajjada', 19994), ('cra', 19993), ('restarted', 19992), ('severus', 19991), ('ngan', 19990), ('magnolia', 19989), ('hornets', 19988), ('symmetric', 19987), ('abolitionist', 19986), ('payoff', 19985), ('lehmann', 19984), ('qasim', 19983), ('dreyer', 19982), ('harkleroad', 19981), ('cain', 19980)]\n"
     ]
    }
   ],
   "source": [
    "# Now, we transform (0), (1), (2), ... to (\"MostCommonWord\", 1)\n",
    "# (\"NextMostCommon\", 2), ...\n",
    "# the number will be the spot in the dictionary used to tell us\n",
    "# where the word is located\n",
    "dictionary = topWordsK.map (lambda x : (topWords[x][0], x))\n",
    "\n",
    "print(\"Word Postions in our Feature Matrix. Last 20 words in 20k positions: \", dictionary.top(20, lambda x : x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frequency position of the word 'applicant' is 346.\n",
      "The frequency position of the word 'and' is 2.\n",
      "The frequency position of the word 'attack' is 502.\n",
      "The frequency position of the word 'protein' is 3014.\n",
      "The frequency position of the word 'car' is 608.\n"
     ]
    }
   ],
   "source": [
    "freq_lookup_words = ['applicant', 'and', 'attack', 'protein', 'car']\n",
    "for w in freq_lookup_words:\n",
    "    result = dictionary.filter(lambda x: w in x).collect()\n",
    "    if len(result) == 0:\n",
    "        print(\"The word '%s' does not appear in the dictionary, so the frequency position of the word is -1.\" % w)\n",
    "    else:\n",
    "        print(\"The frequency position of the word '%s' is %d.\" % (w, result[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Learning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we get a RDD that has, for each (docID, [\"word1\", \"word2\", \"word3\", ...]),\n",
    "# (\"word1\", docID), (\"word2\", docId), ...\n",
    "\n",
    "allWordsWithDocID = d_keyAndListOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now join and link them, to get a set of (\"word1\", (dictionaryPos, docID)) pairs\n",
    "allDictionaryWords = dictionary.join(allWordsWithDocID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AU35', 1), ('AU35', 1), ('AU35', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we drop the actual word itself to get a set of (docID, dictionaryPos) pairs\n",
    "justDocAndPos = allDictionaryWords.map(lambda x: (x[1][1],x[1][0]))\n",
    "justDocAndPos.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "allDictionaryWordsInEachDoc = justDocAndPos.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildArray(listOfIndices):\n",
    "    \n",
    "    returnVal = np.zeros(20000)\n",
    "    \n",
    "    for index in listOfIndices:\n",
    "        returnVal[index] = returnVal[index] + 1\n",
    "    \n",
    "    mysum = np.sum(returnVal)\n",
    "    \n",
    "    returnVal = np.divide(returnVal, mysum)\n",
    "    \n",
    "    return returnVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert regular array to SparseVector\n",
    "def convert_array_to_sparse(my_array):\n",
    "    index = [i for i, e in enumerate(my_array) if e != 0]\n",
    "    value = [e for i, e in enumerate(my_array) if e != 0]\n",
    "    size = len(my_array)\n",
    "    my_sparse_vector = SparseVector(size, index, value)\n",
    "    return my_sparse_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The following line this gets us a set of\n",
    "# (docID,  [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "# and converts the dictionary positions to a bag-of-words numpy array...\n",
    "\n",
    "# sparse version\n",
    "#allDocsAsNumpyArrays = allDictionaryWordsInEachDoc.map(lambda x: LabeledPoint(x[0].startswith(\"AU\")*1,\\\n",
    "#                                                                 convert_array_to_sparse(buildArray(x[1]))))\n",
    "\n",
    "# dense version\n",
    "allDocsAsNumpyArrays = allDictionaryWordsInEachDoc.map(lambda x: (x[0].startswith(\"AU\")*1,buildArray(x[1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = allDocsAsNumpyArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do gradient Decent on our RDD data set. \n",
    "\n",
    "learningRate = 150\n",
    "num_iteration = 100\n",
    "maxIteration = 400\n",
    "\n",
    "precision = 0.01\n",
    "lamb = 0\n",
    "\n",
    "beta = np.zeros(20000)\n",
    "oldCost = 0\n",
    "\n",
    "## convert beta to sparse beta for sparse vector dot product\n",
    "#beta_len = len(beta)\n",
    "#beta_index = list(range(len(beta)))\n",
    "#sparse_beta = SparseVector(beta_len, beta_index, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "initCost = myRDD.map(lambda x: (x[0], x[1], np.dot(x[1], beta)))\\\n",
    "                .map(lambda x: (x[1]*(-x[0]+(np.exp(x[2])/(1+np.exp(x[2])))), \\\n",
    "                                -x[0]*x[2]+np.log(1+np.exp(x[2]))))\\\n",
    "                .reduce(lambda x, y: (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initCost = myRDD.map(lambda x: (x.label, x.features, x.features.dot(sparse_beta)))\\\n",
    "#                .map(lambda x: (x[1]*(-x[0]+(np.exp(x[2])/(1+np.exp(x[2])))), \\\n",
    "#                                -x[0]*x[2]+np.log(1+np.exp(x[2]))))\\\n",
    "#                .reduce(lambda x, y: (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Beta [0. 0. 0. ... 0. 0. 0.]  Cost 2385.812595487337\n",
      "1 Beta [-5.62052152e+00 -2.62399112e+00 -2.38145596e+00 ... -5.12598884e-04\n",
      " -4.19910267e-04 -2.11420316e-04]  Cost 1301.7109596357272\n",
      "2 Beta [-8.79142465e+00 -4.10864262e+00 -3.78423993e+00 ... -8.40945617e-04\n",
      " -6.65994427e-04 -3.23545629e-04]  Cost 926.5224584174243\n",
      "3 Beta [-1.08788523e+01 -5.08848711e+00 -4.74660007e+00 ... -1.08185224e-03\n",
      " -8.36955886e-04 -3.94588253e-04]  Cost 752.2549348757707\n",
      "4 Beta [-1.24034520e+01 -5.80563251e+00 -5.47656983e+00 ... -1.27438329e-03\n",
      " -9.69221204e-04 -4.45646586e-04]  Cost 654.494562324634\n",
      "5 Beta [-1.35884552e+01 -6.36403285e+00 -6.06463776e+00 ... -1.43631460e-03\n",
      " -1.07812039e-03 -4.85269178e-04]  Cost 593.0292017025096\n",
      "6 Beta [-1.45473485e+01 -6.81660411e+00 -6.55733747e+00 ... -1.57712019e-03\n",
      " -1.17139515e-03 -5.17597697e-04]  Cost 551.3761069254648\n",
      "7 Beta [-1.53452412e+01 -7.19373917e+00 -6.98161384e+00 ... -1.70244382e-03\n",
      " -1.25348319e-03 -5.44914196e-04]  Cost 521.6170761455596\n",
      "8 Beta [-1.60228318e+01 -7.51445460e+00 -7.35443852e+00 ... -1.81592317e-03\n",
      " -1.32716365e-03 -5.68596909e-04]  Cost 499.50360008373355\n",
      "9 Beta [-1.66072207e+01 -7.79142421e+00 -7.68718575e+00 ... -1.92004333e-03\n",
      " -1.39429314e-03 -5.89536677e-04]  Cost 482.56381403503343\n",
      "10 Beta [-1.71173481e+01 -8.03351240e+00 -7.98784914e+00 ... -2.01657939e-03\n",
      " -1.45617420e-03 -6.08340060e-04]  Cost 469.2678419549043\n",
      "11 Beta [-1.75669658e+01 -8.24715981e+00 -8.26226082e+00 ... -2.10684464e-03\n",
      " -1.51375650e-03 -6.25437373e-04]  Cost 458.62157741735115\n",
      "12 Beta [-1.79663712e+01 -8.43719224e+00 -8.51480667e+00 ... -2.19183887e-03\n",
      " -1.56775406e-03 -6.41144209e-04]  Cost 449.9531417678875\n",
      "13 Beta [-1.83234733e+01 -8.60731838e+00 -8.74886838e+00 ... -2.27234123e-03\n",
      " -1.61871762e-03 -6.55698456e-04]  Cost 442.79347224450544\n",
      "14 Beta [-1.86444779e+01 -8.76044974e+00 -8.96710848e+00 ... -2.34897113e-03\n",
      " -1.66708097e-03 -6.69283601e-04]  Cost 436.80607548498796\n",
      "15 Beta [-1.89343431e+01 -8.89891365e+00 -9.17166098e+00 ... -2.42222928e-03\n",
      " -1.71319209e-03 -6.82043964e-04]  Cost 431.7439166790045\n",
      "16 Beta [-1.91970927e+01 -9.02459971e+00 -9.36426267e+00 ... -2.49252635e-03\n",
      " -1.75733437e-03 -6.94094987e-04]  Cost 427.42199511991015\n",
      "17 Beta [-1.94360368e+01 -9.13906298e+00 -9.54634605e+00 ... -2.56020349e-03\n",
      " -1.79974178e-03 -7.05530375e-04]  Cost 423.69935225820507\n",
      "18 Beta [-1.96539307e+01 -9.24359853e+00 -9.71910650e+00 ... -2.62554718e-03\n",
      " -1.84060975e-03 -7.16427172e-04]  Cost 420.4669477715482\n",
      "19 Beta [-1.98530927e+01 -9.33929627e+00 -9.88355195e+00 ... -2.68880042e-03\n",
      " -1.88010319e-03 -7.26849449e-04]  Cost 417.639295881939\n",
      "20 Beta [-2.00354913e+01 -9.42708205e+00 -1.00405401e+01 ... -2.75017113e-03\n",
      " -1.91836260e-03 -7.36851029e-04]  Cost 415.14857481250584\n",
      "21 Beta [-2.02028123e+01 -9.50774903e+00 -1.01908069e+01 ... -2.80983859e-03\n",
      " -1.95550861e-03 -7.46477539e-04]  Cost 412.9404008116111\n",
      "22 Beta [-2.03565102e+01 -9.58198176e+00 -1.03349884e+01 ... -2.86795850e-03\n",
      " -1.99164562e-03 -7.55767976e-04]  Cost 410.9707458358818\n",
      "23 Beta [-2.04978489e+01 -9.65037519e+00 -1.04736384e+01 ... -2.92466695e-03\n",
      " -2.02686455e-03 -7.64755917e-04]  Cost 409.20365566017887\n",
      "24 Beta [-2.06279330e+01 -9.71344954e+00 -1.06072414e+01 ... -2.98008356e-03\n",
      " -2.06124509e-03 -7.73470471e-04]  Cost 407.6095376237397\n",
      "25 Beta [-2.07477341e+01 -9.77166233e+00 -1.07362243e+01 ... -3.03431407e-03\n",
      " -2.09485746e-03 -7.81937023e-04]  Cost 406.16385995530595\n",
      "26 Beta [-2.08581104e+01 -9.82541801e+00 -1.08609649e+01 ... -3.08745240e-03\n",
      " -2.12776388e-03 -7.90177843e-04]  Cost 404.84615261165027\n",
      "27 Beta [-2.09598243e+01 -9.87507582e+00 -1.09817990e+01 ... -3.13958230e-03\n",
      " -2.16001968e-03 -7.98212565e-04]  Cost 403.63923180777\n",
      "28 Beta [-2.10535560e+01 -9.92095627e+00 -1.10990266e+01 ... -3.19077882e-03\n",
      " -2.19167436e-03 -8.06058583e-04]  Cost 402.5285924415735\n",
      "29 Beta [-2.11399142e+01 -9.96334649e+00 -1.12129167e+01 ... -3.24110946e-03\n",
      " -2.22277230e-03 -8.13731376e-04]  Cost 401.5019278901426\n",
      "30 Beta [-2.12194467e+01 -1.00025046e+01 -1.13237112e+01 ... -3.29063510e-03\n",
      " -2.25335349e-03 -8.21244775e-04]  Cost 400.5487473965976\n",
      "31 Beta [-2.12926473e+01 -1.00386637e+01 -1.14316288e+01 ... -3.33941088e-03\n",
      " -2.28345407e-03 -8.28611186e-04]  Cost 399.6600689194379\n",
      "32 Beta [-2.13599631e+01 -1.00720346e+01 -1.15368676e+01 ... -3.38748689e-03\n",
      " -2.31310679e-03 -8.35841774e-04]  Cost 398.828170833995\n",
      "33 Beta [-2.14218002e+01 -1.01028088e+01 -1.16396075e+01 ... -3.43490872e-03\n",
      " -2.34234146e-03 -8.42946626e-04]  Cost 398.0463898985821\n",
      "34 Beta [-2.14785281e+01 -1.01311608e+01 -1.17400126e+01 ... -3.48171804e-03\n",
      " -2.37118523e-03 -8.49934878e-04]  Cost 397.3089558616382\n",
      "35 Beta [-2.15304843e+01 -1.01572498e+01 -1.18382327e+01 ... -3.52795297e-03\n",
      " -2.39966295e-03 -8.56814833e-04]  Cost 396.61085529081004\n",
      "36 Beta [-2.15779776e+01 -1.01812216e+01 -1.19344051e+01 ... -3.57364850e-03\n",
      " -2.42779737e-03 -8.63594053e-04]  Cost 395.94771885988786\n",
      "37 Beta [-2.16212915e+01 -1.02032100e+01 -1.20286558e+01 ... -3.61883682e-03\n",
      " -2.45560941e-03 -8.70279447e-04]  Cost 395.3157275825036\n",
      "38 Beta [-2.16606865e+01 -1.02233380e+01 -1.21211007e+01 ... -3.66354759e-03\n",
      " -2.48311831e-03 -8.76877342e-04]  Cost 394.71153443774756\n",
      "39 Beta [-2.16964026e+01 -1.02417189e+01 -1.22118467e+01 ... -3.70780819e-03\n",
      " -2.51034184e-03 -8.83393544e-04]  Cost 394.13219856825447\n",
      "40 Beta [-2.17286615e+01 -1.02584574e+01 -1.23009927e+01 ... -3.75164396e-03\n",
      " -2.53729639e-03 -8.89833391e-04]  Cost 393.5751298007773\n",
      "41 Beta [-2.17576682e+01 -1.02736504e+01 -1.23886299e+01 ... -3.79507839e-03\n",
      " -2.56399716e-03 -8.96201804e-04]  Cost 393.03804168337285\n",
      "42 Beta [-2.17836125e+01 -1.02873876e+01 -1.24748431e+01 ... -3.83813325e-03\n",
      " -2.59045824e-03 -9.02503327e-04]  Cost 392.5189115817559\n",
      "43 Beta [-2.18066709e+01 -1.02997522e+01 -1.25597110e+01 ... -3.88082883e-03\n",
      " -2.61669272e-03 -9.08742163e-04]  Cost 392.0159466524672\n",
      "44 Beta [-2.18270070e+01 -1.03108217e+01 -1.26433066e+01 ... -3.92318397e-03\n",
      " -2.64271278e-03 -9.14922206e-04]  Cost 391.5275547289363\n",
      "45 Beta [-2.18447736e+01 -1.03206681e+01 -1.27256982e+01 ... -3.96521627e-03\n",
      " -2.66852976e-03 -9.21047072e-04]  Cost 391.05231933088277\n",
      "46 Beta [-2.18601128e+01 -1.03293586e+01 -1.28069492e+01 ... -4.00694215e-03\n",
      " -2.69415424e-03 -9.27120119e-04]  Cost 390.58897814743773\n",
      "47 Beta [-2.18731574e+01 -1.03369561e+01 -1.28871190e+01 ... -4.04837695e-03\n",
      " -2.71959612e-03 -9.33144475e-04]  Cost 390.13640445722706\n",
      "48 Beta [-2.18840314e+01 -1.03435191e+01 -1.29662631e+01 ... -4.08953503e-03\n",
      " -2.74486463e-03 -9.39123057e-04]  Cost 389.69359104008515\n",
      "49 Beta [-2.18928511e+01 -1.03491025e+01 -1.30444333e+01 ... -4.13042985e-03\n",
      " -2.76996844e-03 -9.45058587e-04]  Cost 389.2596362095147\n",
      "50 Beta [-2.18997251e+01 -1.03537578e+01 -1.31216784e+01 ... -4.17107403e-03\n",
      " -2.79491567e-03 -9.50953608e-04]  Cost 388.8337316558345\n",
      "51 Beta [-2.19047556e+01 -1.03575332e+01 -1.31980441e+01 ... -4.21147940e-03\n",
      " -2.81971394e-03 -9.56810501e-04]  Cost 388.4151518399077\n",
      "52 Beta [-2.19080384e+01 -1.03604741e+01 -1.32735731e+01 ... -4.25165711e-03\n",
      " -2.84437040e-03 -9.62631496e-04]  Cost 388.00324471849297\n",
      "53 Beta [-2.19096636e+01 -1.03626229e+01 -1.33483059e+01 ... -4.29161763e-03\n",
      " -2.86889178e-03 -9.68418685e-04]  Cost 387.59742361629947\n",
      "54 Beta [-2.19097158e+01 -1.03640198e+01 -1.34222803e+01 ... -4.33137081e-03\n",
      " -2.89328441e-03 -9.74174030e-04]  Cost 387.1971600880922\n",
      "55 Beta [-2.19082749e+01 -1.03647025e+01 -1.34955320e+01 ... -4.37092594e-03\n",
      " -2.91755427e-03 -9.79899378e-04]  Cost 386.8019776377387\n",
      "56 Beta [-2.19054159e+01 -1.03647065e+01 -1.35680948e+01 ... -4.41029177e-03\n",
      " -2.94170696e-03 -9.85596463e-04]  Cost 386.4114461807588\n",
      "57 Beta [-2.19012100e+01 -1.03640654e+01 -1.36400004e+01 ... -4.44947658e-03\n",
      " -2.96574780e-03 -9.91266920e-04]  Cost 386.02517715345317\n",
      "58 Beta [-2.18957238e+01 -1.03628108e+01 -1.37112787e+01 ... -4.48848815e-03\n",
      " -2.98968180e-03 -9.96912287e-04]  Cost 385.64281918556225\n",
      "59 Beta [-2.18890207e+01 -1.03609726e+01 -1.37819581e+01 ... -4.52733387e-03\n",
      " -3.01351369e-03 -1.00253402e-03]  Cost 385.26405426511013\n",
      "60 Beta [-2.18811605e+01 -1.03585792e+01 -1.38520654e+01 ... -4.56602071e-03\n",
      " -3.03724794e-03 -1.00813348e-03]  Cost 384.8885943340094\n",
      "61 Beta [-2.18721996e+01 -1.03556571e+01 -1.39216259e+01 ... -4.60455526e-03\n",
      " -3.06088879e-03 -1.01371197e-03]  Cost 384.51617826141063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 Beta [-2.18621916e+01 -1.03522319e+01 -1.39906635e+01 ... -4.64294377e-03\n",
      " -3.08444025e-03 -1.01927071e-03]  Cost 384.1465691489094\n",
      "63 Beta [-2.18511872e+01 -1.03483273e+01 -1.40592010e+01 ... -4.68119215e-03\n",
      " -3.10790610e-03 -1.02481085e-03]  Cost 383.779551927851\n",
      "64 Beta [-2.18392343e+01 -1.03439662e+01 -1.41272599e+01 ... -4.71930601e-03\n",
      " -3.13128996e-03 -1.03033350e-03]  Cost 383.4149312141742\n",
      "65 Beta [-2.18263785e+01 -1.03391700e+01 -1.41948605e+01 ... -4.75729067e-03\n",
      " -3.15459523e-03 -1.03583968e-03]  Cost 383.0525293907027\n",
      "66 Beta [-2.18126631e+01 -1.03339593e+01 -1.42620224e+01 ... -4.79515116e-03\n",
      " -3.17782515e-03 -1.04133038e-03]  Cost 382.6921848906399\n",
      "67 Beta [-2.17981289e+01 -1.03283535e+01 -1.43287637e+01 ... -4.83289228e-03\n",
      " -3.20098279e-03 -1.04680653e-03]  Cost 382.33375065932273\n",
      "68 Beta [-2.17828151e+01 -1.03223709e+01 -1.43951021e+01 ... -4.87051856e-03\n",
      " -3.22407107e-03 -1.05226901e-03]  Cost 381.9770927741512\n",
      "69 Beta [-2.17667585e+01 -1.03160290e+01 -1.44610539e+01 ... -4.90803433e-03\n",
      " -3.24709277e-03 -1.05771866e-03]  Cost 381.6220892050593\n",
      "70 Beta [-2.17499943e+01 -1.03093446e+01 -1.45266352e+01 ... -4.94544369e-03\n",
      " -3.27005052e-03 -1.06315627e-03]  Cost 381.2686287000563\n",
      "71 Beta [-2.17325560e+01 -1.03023335e+01 -1.45918607e+01 ... -4.98275054e-03\n",
      " -3.29294682e-03 -1.06858260e-03]  Cost 380.91660978219113\n",
      "72 Beta [-2.17144755e+01 -1.02950106e+01 -1.46567448e+01 ... -5.01995858e-03\n",
      " -3.31578406e-03 -1.07399837e-03]  Cost 380.5659398459412\n",
      "73 Beta [-2.16957829e+01 -1.02873904e+01 -1.47213010e+01 ... -5.05707136e-03\n",
      " -3.33856452e-03 -1.07940426e-03]  Cost 380.2165343423964\n",
      "74 Beta [-2.16765072e+01 -1.02794865e+01 -1.47855422e+01 ... -5.09409222e-03\n",
      " -3.36129034e-03 -1.08480091e-03]  Cost 379.8683160438666\n",
      "75 Beta [-2.16566758e+01 -1.02713119e+01 -1.48494809e+01 ... -5.13102437e-03\n",
      " -3.38396359e-03 -1.09018895e-03]  Cost 379.52121437960227\n",
      "76 Beta [-2.16363148e+01 -1.02628790e+01 -1.49131286e+01 ... -5.16787084e-03\n",
      " -3.40658622e-03 -1.09556896e-03]  Cost 379.17516483525077\n",
      "77 Beta [-2.16154492e+01 -1.02541996e+01 -1.49764965e+01 ... -5.20463454e-03\n",
      " -3.42916011e-03 -1.10094150e-03]  Cost 378.83010840950897\n",
      "78 Beta [-2.15941028e+01 -1.02452849e+01 -1.50395953e+01 ... -5.24131824e-03\n",
      " -3.45168704e-03 -1.10630711e-03]  Cost 378.48599112215516\n",
      "79 Beta [-2.15722983e+01 -1.02361457e+01 -1.51024352e+01 ... -5.27792456e-03\n",
      " -3.47416869e-03 -1.11166628e-03]  Cost 378.1427635682724\n",
      "80 Beta [-2.15500572e+01 -1.02267922e+01 -1.51650259e+01 ... -5.31445601e-03\n",
      " -3.49660669e-03 -1.11701951e-03]  Cost 377.80038051404426\n",
      "81 Beta [-2.15274003e+01 -1.02172342e+01 -1.52273765e+01 ... -5.35091500e-03\n",
      " -3.51900259e-03 -1.12236725e-03]  Cost 377.45880053000457\n",
      "82 Beta [-2.15043472e+01 -1.02074810e+01 -1.52894959e+01 ... -5.38730380e-03\n",
      " -3.54135786e-03 -1.12770994e-03]  Cost 377.1179856580589\n",
      "83 Beta [-2.14809167e+01 -1.01975416e+01 -1.53513925e+01 ... -5.42362459e-03\n",
      " -3.56367390e-03 -1.13304801e-03]  Cost 376.77790110896876\n",
      "84 Beta [-2.14571267e+01 -1.01874244e+01 -1.54130744e+01 ... -5.45987945e-03\n",
      " -3.58595205e-03 -1.13838184e-03]  Cost 376.43851498736365\n",
      "85 Beta [-2.14329945e+01 -1.01771376e+01 -1.54745492e+01 ... -5.49607035e-03\n",
      " -3.60819361e-03 -1.14371182e-03]  Cost 376.0997980416273\n",
      "86 Beta [-2.14085365e+01 -1.01666889e+01 -1.55358243e+01 ... -5.53219921e-03\n",
      " -3.63039979e-03 -1.14903832e-03]  Cost 375.76172343628394\n",
      "87 Beta [-2.13837682e+01 -1.01560858e+01 -1.55969067e+01 ... -5.56826781e-03\n",
      " -3.65257176e-03 -1.15436167e-03]  Cost 375.42426654475383\n",
      "88 Beta [-2.13587046e+01 -1.01453353e+01 -1.56578031e+01 ... -5.60427788e-03\n",
      " -3.67471064e-03 -1.15968222e-03]  Cost 375.08740476055954\n",
      "89 Beta [-2.13333601e+01 -1.01344442e+01 -1.57185198e+01 ... -5.64023108e-03\n",
      " -3.69681751e-03 -1.16500027e-03]  Cost 374.75111732525943\n",
      "90 Beta [-2.13077483e+01 -1.01234190e+01 -1.57790631e+01 ... -5.67612898e-03\n",
      " -3.71889337e-03 -1.17031612e-03]  Cost 374.4153851715531\n",
      "91 Beta [-2.12818823e+01 -1.01122659e+01 -1.58394387e+01 ... -5.71197306e-03\n",
      " -3.74093921e-03 -1.17563007e-03]  Cost 374.0801907801542\n",
      "92 Beta [-2.12557747e+01 -1.01009908e+01 -1.58996522e+01 ... -5.74776479e-03\n",
      " -3.76295596e-03 -1.18094239e-03]  Cost 373.7455180491798\n",
      "93 Beta [-2.12294373e+01 -1.00895995e+01 -1.59597091e+01 ... -5.78350551e-03\n",
      " -3.78494451e-03 -1.18625334e-03]  Cost 373.4113521748931\n",
      "94 Beta [-2.12028816e+01 -1.00780972e+01 -1.60196144e+01 ... -5.81919655e-03\n",
      " -3.80690571e-03 -1.19156317e-03]  Cost 373.0776795427903\n",
      "95 Beta [-2.11761186e+01 -1.00664893e+01 -1.60793731e+01 ... -5.85483915e-03\n",
      " -3.82884039e-03 -1.19687212e-03]  Cost 372.74448762808527\n",
      "96 Beta [-2.11491588e+01 -1.00547807e+01 -1.61389898e+01 ... -5.89043452e-03\n",
      " -3.85074931e-03 -1.20218042e-03]  Cost 372.4117649047546\n",
      "97 Beta [-2.11220122e+01 -1.00429761e+01 -1.61984690e+01 ... -5.92598379e-03\n",
      " -3.87263324e-03 -1.20748828e-03]  Cost 372.07950076237137\n",
      "98 Beta [-2.10946884e+01 -1.00310801e+01 -1.62578151e+01 ... -5.96148806e-03\n",
      " -3.89449288e-03 -1.21279592e-03]  Cost 371.74768543004876\n",
      "99 Beta [-2.10671966e+01 -1.00190971e+01 -1.63170320e+01 ... -5.99694837e-03\n",
      " -3.91632891e-03 -1.21810353e-03]  Cost 371.41630990684615\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# two variables for visualizing the cost\n",
    "xx, costHist = [], [] \n",
    "\n",
    "fig = plt.figure(figsize=(10,6), dpi= 100, facecolor='w', edgecolor='k')\n",
    "ax2 = plt.subplot(1, 1, 1)\n",
    "ax2.set_xlim(left = 0, right=num_iteration)\n",
    "ax2.set_ylim(bottom = 0, top=initCost[1])\n",
    "\n",
    "# Let's start with main iterative part of gradient descent algorithm \n",
    "for i in range(num_iteration):\n",
    "\n",
    "    gradientCost = myRDD.map(lambda x: (x[0], x[1], np.dot(x[1], beta)))\\\n",
    "                        .map(lambda x: (-x[1]*x[0]+x[1]*(np.exp(x[2])/(1+np.exp(x[2]))), \\\n",
    "                                        -x[0]*x[2]+np.log(1+np.exp(x[2]))))\\\n",
    "                        .reduce(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "    \n",
    "    cost = gradientCost[1] + lamb*(norm(beta)**2)\n",
    "    \n",
    "    gradient = (1/float(size)) * (gradientCost[0] + 2*lamb*beta)\n",
    "    \n",
    "    # Stop if the cost is not descreasing \n",
    "    if(abs(cost - oldCost) <= precision):\n",
    "        print(\"Stopped at iteration\", i)\n",
    "        break\n",
    "        \n",
    "    if(i >= maxIteration):\n",
    "        print(\"Reach maximum iteration allowed, stopped at iteration\", maxIteration)\n",
    "        break\n",
    "\n",
    "    oldCost = cost\n",
    "    \n",
    "    # Visualization     \n",
    "    # Update the Cost Diagram. \n",
    "    xx.append(i)\n",
    "    costHist.append(cost)    \n",
    "    ax2.plot(xx, costHist, color='blue')\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    print(i, \"Beta\", beta, \" Cost\", cost)\n",
    "    beta = beta - learningRate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('morelos', 19999),\n",
       " ('odisha', 19998),\n",
       " ('chittagong', 19997),\n",
       " ('sirte', 19996),\n",
       " ('fiqh', 19995)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the top 5 words with the largest regression coefficients (in descending order)\n",
    "top5_beta = np.sort(beta)[::-1][:5]\n",
    "top5_beta_index = [np.where(beta == x)[0][0] for x in top5_beta]\n",
    "[dictionary.filter(lambda x: y in x).collect()[0] for y in top5_beta_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 1 word with large coefficient is morelos\n",
      "top 2 word with large coefficient is odisha\n",
      "top 3 word with large coefficient is chittagong\n",
      "top 4 word with large coefficient is sirte\n",
      "top 5 word with large coefficient is fiqh\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    w = top5_beta_index[i]\n",
    "    w_tuple = dictionary.filter(lambda x: w in x).collect()[0]\n",
    "    print(\"top %d word with large coefficient is %s\" % (i+1, w_tuple[0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9785008715862871"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = myRDD.map(lambda x: (x[0], np.dot(x[1], beta)))\\\n",
    "                               .map(lambda x: (x[0], np.exp(x[1])/(1+np.exp(x[1]))))\\\n",
    "                               .map(lambda x: (x[1] > 0.5)*1 == x[0]).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegularization(myRDD, my_lambda,\\\n",
    "                           learningRate = 150,\\\n",
    "                           num_iteration = 100,\\\n",
    "                           maxIteration = 400,\\\n",
    "                           precision = 0.01):\n",
    "\n",
    "    size = myRDD.count()\n",
    "    beta = np.zeros(20000)\n",
    "    oldCost = 0\n",
    "    \n",
    "    # Let's start with main iterative part of gradient descent algorithm \n",
    "    for i in range(num_iteration):\n",
    "\n",
    "        gradientCost = myRDD.map(lambda x: (x[0], x[1], np.dot(x[1], beta)))\\\n",
    "                            .map(lambda x: (-x[1]*x[0]+x[1]*(np.exp(x[2])/(1+np.exp(x[2]))), \\\n",
    "                                            -x[0]*x[2]+np.log(1+np.exp(x[2]))))\\\n",
    "                            .reduce(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "\n",
    "        cost = gradientCost[1] + my_lambda*(norm(beta)**2)\n",
    "\n",
    "        gradient = (1/float(size)) * (gradientCost[0] + 2*my_lambda*beta)\n",
    "\n",
    "        # Stop if the cost is not descreasing \n",
    "        if(abs(cost - oldCost) <= precision):\n",
    "            print(\"Stopped at iteration\", i)\n",
    "            break\n",
    "\n",
    "        if(i >= maxIteration):\n",
    "            print(\"Reach maximum iteration allowed, stopped at iteration\", maxIteration)\n",
    "            break\n",
    "\n",
    "        oldCost = cost\n",
    "        beta = beta - learningRate * gradient\n",
    "        \n",
    "    accuracy = myRDD.map(lambda x: (x[0], np.dot(x[1], beta)))\\\n",
    "                               .map(lambda x: (x[0], np.exp(x[1])/(1+np.exp(x[1]))))\\\n",
    "                               .map(lambda x: (x[1] > 0.5)*1 == x[0]).mean()\n",
    "    return accuracy, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped at iteration 12\n",
      "Stopped at iteration 6\n"
     ]
    }
   ],
   "source": [
    "# run on a set of lambdas\n",
    "lambda_candidates = [0, 2, 4, 6, 8, 10]\n",
    "accuracy_list = []\n",
    "beta_list = []\n",
    "for x in lambda_candidates:\n",
    "    current_accuracy, current_beta = LogisticRegularization(myRDD, x)\n",
    "    accuracy_list.append(current_accuracy)\n",
    "    beta_list.append(current_beta)\n",
    "    \n",
    "    # the top 5 words with the largest regression coefficients (in descending order)\n",
    "    top5_current_beta = np.sort(current_beta)[::-1][:5]\n",
    "    top5_current_beta_index = [np.where(current_beta == x)[0][0] for x in top5_best_beta]\n",
    "    \n",
    "    print(\"Current testing lambda is %.2lf with accuracy %.4lf, the top 5 words are given below.\" % (x, current_accuracy))\n",
    "    for i in range(5):\n",
    "        w = top5_current_beta_index[i]\n",
    "        w_tuple = dictionary.filter(lambda x: w in x).collect()[0]\n",
    "        print(\"top %d word with large coefficient is %s\" % (i+1, w_tuple[0]))    \n",
    "\n",
    "# select the best lambda yield the highest accuracy\n",
    "best_accuracy_index = accuracy_list.index(max(accuracy_list))\n",
    "best_beta = beta_list[best_accuracy_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best regularization value is 2.00 with accuracy 0.9785.\n"
     ]
    }
   ],
   "source": [
    "print(\"The best regularization value is %.2lf with accuracy %.4lf.\" % \\\n",
    "      (lambda_candidates[best_accuracy_index], \\\n",
    "       accuracy_list[best_accuracy_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 1 word with large coefficient is applicant\n",
      "top 2 word with large coefficient is tribunal\n",
      "top 3 word with large coefficient is mr\n",
      "top 4 word with large coefficient is respondent\n",
      "top 5 word with large coefficient is application\n"
     ]
    }
   ],
   "source": [
    "# the top 5 words with the largest regression coefficients (in descending order)\n",
    "top5_best_beta = np.sort(best_beta)[::-1][:5]\n",
    "top5_best_beta_index = [np.where(best_beta == x)[0][0] for x in top5_best_beta]\n",
    "\n",
    "for i in range(5):\n",
    "    w = top5_best_beta_index[i]\n",
    "    w_tuple = dictionary.filter(lambda x: w in x).collect()[0]\n",
    "    print(\"top %d word with large coefficient is %s\" % (i+1, w_tuple[0]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Evaluation of the learned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test_corpus = sc.textFile(\"/Users/pudding/Downloads/TestingData.txt\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we transform it into a set of (docID, text) pairs\n",
    "d_test_keyAndText = d_test_corpus.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all non letter characters\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "d_test_keyAndListOfWords = d_test_keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we get a RDD that has, for each (docID, [\"word1\", \"word2\", \"word3\", ...]),\n",
    "# (\"word1\", docID), (\"word2\", docId), ...\n",
    "\n",
    "test_allWordsWithDocID = d_test_keyAndListOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now join and link them, to get a set of (\"word1\", (dictionaryPos, docID)) pairs\n",
    "test_allDictionaryWords = dictionary.join(test_allWordsWithDocID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('was', (6, 'AU11'))]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_allDictionaryWords.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AU11', 6), ('AU11', 6), ('AU11', 6)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we drop the actual word itself to get a set of (docID, dictionaryPos) pairs\n",
    "test_justDocAndPos = test_allDictionaryWords.map(lambda x: (x[1][1],x[1][0]))\n",
    "test_justDocAndPos.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "test_allDictionaryWordsInEachDoc = test_justDocAndPos.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line this gets us a set of\n",
    "# (docID,  [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "# and converts the dictionary positions to a bag-of-words numpy array...\n",
    "\n",
    "# dense version\n",
    "test_allDocsAsNumpyArrays = test_allDictionaryWordsInEachDoc.map(lambda x: (x[0].startswith(\"AU\")*1,buildArray(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_myRDD = test_allDocsAsNumpyArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_myRDD.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = test_myRDD.map(lambda x: np.dot(x[1], best_beta)).map(lambda x: ((np.exp(x)/(1+np.exp(x)))>1)*1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "f1_score = (2*TP)/(2*TP+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true positive is 0.\n",
      "The true negative is 18347.\n",
      "The false positive is 0.\n",
      "The false negative is 377.\n",
      "The F1 score is 0.0000.\n"
     ]
    }
   ],
   "source": [
    "print(\"The true positive is %d.\" % TP)\n",
    "print(\"The true negative is %d.\" % TN)\n",
    "print(\"The false positive is %d.\" % FP)\n",
    "print(\"The false negative is %d.\" % FN)\n",
    "print(\"The F1 score is %.4lf.\" % f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AU1174', \"bailpending appeal from court of summary jurisdictionwhere appellant has been sentenced to imprisonment for offences under the copyright act 1968 whether the court has jurisdiction to stay or suspend the operation of the sentence pending the hearing of an appeal against the sentenceappealI was approached urgently tonight in this appeal to grant bail to the appellant pending an opportunity for a hearing on whether or not he should be granted bail or the orders for his weekend detention should be stayed.The application has been made ex parte in circumstances where, because of the lateness of the hour, I have conducted the hearing by telephone with the solicitor for the appellant being present on the telephone together with the appellant personally.I am satisfied that I have jurisdiction to make orders as a single judge exercising the appellate jurisdiction pursuant to s 25 of the Federal Court of Australia Act 1976  .For present purposes, and in the absence of any detailed argument, I am satisfied that my powers include a power under s 29 to stay or suspend the operation of the sentence and that if I do so s 29A of the Act provides that the time during which the appellant is released on bail pending the determination of the appeal does not count as part of the term of his imprisonment.Pursuant to O 52 r 35 I have power to admit the appellant to bail pending the hearing of his appeal.I am satisfied that the Court also has inherent jurisdiction, having been seized of jurisdiction to deal with the appeal, to stay the operation of the sentence: Tait v The Queen [1962] HCA 57 ; (1962) 108 CLR 620 at 624-625.The circumstances of this case show that the appellant gave an unconditional bail undertaking to appear at the District Court of New South Wales at Campbelltown at 9.30 am today and thereafter to attend at that court at such day and at such time and place as was, from time to time, specified in a notice to be given or sent to him.This undertaking was given in the context of the appellant, and those advising him at that time, understanding that the appeal from the decision of the Local Court of New South Wales sentencing the appellant to a term of six months' weekend detention lay to the District Court of New South Wales, rather than to this Court pursuant to the provisions of the Copyright Act 1968 .On the basis that the appellant would have been entitled to bail tonight, pending the determination of his appeal by the District Court, it seems to me that there are sufficient circumstances to justify my making an order admitting him to bail and staying the operation of his sentence up to and including next Thursday.Then the matter can be dealt with appropriately by those representing the appellant as well as those representing the respondent, New South Wales Police Service, or any other person who may take over the conduct of the respondent's case on the appeal.The parties will then be able to debate, in an informed way, before the Court the way in which the appeal ought thereafter to proceed, the appellant's entitlement to bail, and the conditions, if any, which ought to attach to any grant of bail that might be appropriate to give to the appellant.In my opinion it would not be just to require the appellant to commence tonight to serve his weekend detention having regard to the circumstances in which the matter has come before the Court that I have set out.In those circumstances I propose, subject to hearing from the solicitor for the appellant as to any comments he has to make as to the form of the orders, to make orders in the terms that I have suggested are appropriate.\")\n"
     ]
    }
   ],
   "source": [
    "if FP == 0:\n",
    "    print(\"There is no False Positive case.\")\n",
    "elif FP == 1:\n",
    "    article_index = np.where((y_true == 0) & (y_pred == 1))[0][0]\n",
    "    print(d_test_keyAndText.take(article_index+1)[article_index])\n",
    "elif FP == 2:\n",
    "    article_index = np.where((y_true == 0) & (y_pred == 1))[0][0]\n",
    "    print(d_test_keyAndText.take(article_index+1)[article_index])   \n",
    "    article_index = np.where((y_true == 0) & (y_pred == 1))[0][1]\n",
    "    print(d_test_keyAndText.take(article_index+1)[article_index])\n",
    "else: \n",
    "    article_index = np.where((y_true == 0) & (y_pred == 1))[0][0]\n",
    "    print(d_test_keyAndText.take(article_index+1)[article_index])   \n",
    "    article_index = np.where((y_true == 0) & (y_pred == 1))[0][1]\n",
    "    print(d_test_keyAndText.take(article_index+1)[article_index])  \n",
    "    article_index = np.where((y_true == 0) & (y_pred == 1))[0][2]\n",
    "    print(d_test_keyAndText.take(article_index+1)[article_index])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
